{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6c5ff-5dd4-4fdc-83ab-4c29edea4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem\n",
    "    print(\"RDKit Imported successfully\")\n",
    "except ImportError:\n",
    "    raise ImportError(\"rdkit: pip install rdkit\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"PyTorch versions: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Graphics card presence\")\n",
    "else:\n",
    "    print(\"Graphics card not present\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "try:\n",
    "    from torch_geometric.data import Data, Batch, Dataset\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "except ImportError:\n",
    "    raise ImportError(\"PyTorch Geometric: pip install torch_geometric\")\n",
    "\n",
    "try:\n",
    "    from pymatgen.core import Structure\n",
    "except ImportError:\n",
    "    raise ImportError(\"Pymatgen: pip install pymatgen\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = r\"./data\"\n",
    "    DATASET_PATH = os.path.join(BASE_PATH, \"Energy_data.xlsx\")\n",
    "    CIF_PATH = os.path.join(BASE_PATH, \"molecular_sieve\")\n",
    "    CONFORMER_3D_PATH = os.path.join(BASE_PATH, \"conformer 3D\")\n",
    "    STRUCTURE_2D_PATH = os.path.join(BASE_PATH, \"structure 2D\")\n",
    "    \n",
    "    PROCESSED_CACHE_PATH = os.path.join(BASE_PATH, \"cached_graphs_box64_cleaned.pt\")\n",
    "    \n",
    "    TARGET_COLS = [\n",
    "        'Binding Energy (kJ/mol Si)',\n",
    "        'Directivity Energy (kJ/mol Si)',\n",
    "        'Competition Energy (kJ/mol Si)',\n",
    "        'Binding Energy (kJ/mol OSDA)',\n",
    "        'Competition Energy (kJ/mol OSDA)'\n",
    "    ]\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 0  \n",
    "    PIN_MEMORY = True\n",
    "\n",
    "    ATOM_EMBEDDING_DIM = 64\n",
    "    HIDDEN_DIM = 128\n",
    "\n",
    "    EMB_DIM_DEGREE = 8\n",
    "    EMB_DIM_CHARGE = 8\n",
    "    EMB_DIM_HYB = 8\n",
    "    EMB_DIM_AROMATIC = 4\n",
    "    EMB_DIM_CHIRAL = 4\n",
    "\n",
    "    LR = 0.0008\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    EPOCHS = 200\n",
    "    CRYSTAL_RADIUS = 6.0\n",
    "\n",
    "    VOXEL_SIZE = 64  \n",
    "    VOXEL_RES = 0.5\n",
    "    SIGMA = 0.5\n",
    "    \n",
    "    # Data clarity\n",
    "    MIN_SAMPLES_PER_TOPO = 0\n",
    "\n",
    "    # Early Stop Configuration\n",
    "    EARLY_STOPPING_PATIENCE = 20\n",
    "    EARLY_STOPPING_MIN_DELTA = 0.001\n",
    "\n",
    "    # Learning Rate Scheduling Configuration\n",
    "    SCHEDULER_PATIENCE = 5\n",
    "    SCHEDULER_FACTOR = 0.5\n",
    "    MIN_LR = 1e-6\n",
    "    \n",
    "    # RDKit Number of conformations generated\n",
    "    NUM_CONFORMERS = 3\n",
    "\n",
    "# 2. 3D Transformations and Voxelization\n",
    "def get_random_rotation_matrix():\n",
    "    \"\"\"Generate random 3D rotation matrices\"\"\"\n",
    "    theta = np.random.uniform(0, 2*np.pi)\n",
    "    phi = np.random.uniform(0, 2*np.pi)\n",
    "    z = np.random.uniform(0, 2*np.pi)\n",
    "\n",
    "    Rx = np.array([[1, 0, 0],\n",
    "                   [0, np.cos(theta), -np.sin(theta)],\n",
    "                   [0, np.sin(theta),  np.cos(theta)]])\n",
    "\n",
    "    Ry = np.array([[np.cos(phi), 0, np.sin(phi)],\n",
    "                   [0, 1, 0],\n",
    "                   [-np.sin(phi), 0, np.cos(phi)]])\n",
    "\n",
    "    Rz = np.array([[np.cos(z), -np.sin(z), 0],\n",
    "                   [np.sin(z),  np.cos(z), 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "    return Rz @ Ry @ Rx\n",
    "\n",
    "def coords_to_voxel(coords, grid_size=32, res=0.5, sigma=0.5):\n",
    "    grid = np.zeros((grid_size, grid_size, grid_size), dtype=np.float32)\n",
    "    limit = (grid_size * res) / 2.0\n",
    "\n",
    "    mask = (coords[:, 0] > -limit) & (coords[:, 0] < limit) & \\\n",
    "           (coords[:, 1] > -limit) & (coords[:, 1] < limit) & \\\n",
    "           (coords[:, 2] > -limit) & (coords[:, 2] < limit)\n",
    "\n",
    "    valid_coords = coords[mask]\n",
    "    if len(valid_coords) == 0:\n",
    "        return grid\n",
    "\n",
    "    indices = ((valid_coords + limit) / res).astype(int)\n",
    "    indices = np.clip(indices, 0, grid_size - 1)\n",
    "\n",
    "    for idx in indices:\n",
    "        x, y, z = idx\n",
    "        x_min, x_max = max(0, x-1), min(grid_size, x+2)\n",
    "        y_min, y_max = max(0, y-1), min(grid_size, y+2)\n",
    "        z_min, z_max = max(0, z-1), min(grid_size, z+2)\n",
    "        grid[x_min:x_max, y_min:y_max, z_min:z_max] += 1.0\n",
    "\n",
    "    return np.clip(grid, 0, 1.0)\n",
    "\n",
    "# 3. graphical construction\n",
    "class GraphBuilder:\n",
    "    def _get_atom_encoding_legacy(self, atomic_num):\n",
    "        if atomic_num > 100: return 100\n",
    "        return atomic_num - 1\n",
    "\n",
    "    def _get_rich_atom_features(self, atom=None, element_symbol=None, is_crystal=False):\n",
    "\n",
    "        # 1. Atomic Number (0-118)\n",
    "        if atom:\n",
    "            atomic_num = atom.GetAtomicNum()\n",
    "        elif element_symbol:\n",
    "            pt = Chem.GetPeriodicTable()\n",
    "            atomic_num = pt.GetAtomicNumber(element_symbol)\n",
    "        else:\n",
    "            atomic_num = 0\n",
    "\n",
    "        feat_atomic = min(atomic_num, 118)\n",
    "\n",
    "        if is_crystal or atom is None:\n",
    "            return [feat_atomic, 0, 5, 0, 0, 0] \n",
    "\n",
    "        # 2. Degree (0-10)\n",
    "        degree = min(atom.GetDegree(), 10)\n",
    "\n",
    "        # 3. Formal Charge\n",
    "        charge = atom.GetFormalCharge()\n",
    "        charge_idx = charge + 5 \n",
    "        charge_idx = max(0, min(charge_idx, 14))\n",
    "\n",
    "        # 4. Hybridization (0-6)\n",
    "        hyb = atom.GetHybridization()\n",
    "        hyb_map = {\n",
    "            Chem.rdchem.HybridizationType.S: 0,\n",
    "            Chem.rdchem.HybridizationType.SP: 1,\n",
    "            Chem.rdchem.HybridizationType.SP2: 2,\n",
    "            Chem.rdchem.HybridizationType.SP3: 3,\n",
    "            Chem.rdchem.HybridizationType.SP3D: 4,\n",
    "            Chem.rdchem.HybridizationType.SP3D2: 5,\n",
    "            Chem.rdchem.HybridizationType.UNSPECIFIED: 6\n",
    "        }\n",
    "        hyb_idx = hyb_map.get(hyb, 6)\n",
    "\n",
    "        # 5. Aromaticity (0 or 1)\n",
    "        is_aromatic = 1 if atom.GetIsAromatic() else 0\n",
    "\n",
    "        # 6. Chirality (0-3)\n",
    "        chi_map = {\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED: 0,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW: 1,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW: 2,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER: 3\n",
    "        }\n",
    "        chi_idx = chi_map.get(atom.GetChiralTag(), 0)\n",
    "\n",
    "        return [feat_atomic, degree, charge_idx, hyb_idx, is_aromatic, chi_idx]\n",
    "\n",
    "    def _calculate_shape_descriptors(self, coords):\n",
    "        if coords is None or len(coords) < 2:\n",
    "            return [0.0] * 10\n",
    "        coords = coords - np.mean(coords, axis=0)\n",
    "        cov_matrix = np.cov(coords.T)\n",
    "        evals, evecs = np.linalg.eigh(cov_matrix)\n",
    "        idx = evals.argsort()[::-1]\n",
    "        evals = evals[idx]\n",
    "        L, W, H = 0.0, 0.0, 0.0\n",
    "        try:\n",
    "            aligned_coords = np.dot(coords, evecs[:, idx])\n",
    "            min_b = np.min(aligned_coords, axis=0)\n",
    "            max_b = np.max(aligned_coords, axis=0)\n",
    "            dims = max_b - min_b\n",
    "            L, W, H = sorted(dims, reverse=True)\n",
    "        except:\n",
    "            pass\n",
    "        rg = np.sqrt(np.mean(np.sum(coords**2, axis=1)))\n",
    "        pm1 = max(evals[0], 1e-6)\n",
    "        pm2 = evals[1] if len(evals) > 1 else 0.0\n",
    "        pm3 = evals[2] if len(evals) > 2 else 0.0\n",
    "        return [\n",
    "            rg,\n",
    "            pm1, pm2, pm3,\n",
    "            L/(H+1e-6),\n",
    "            np.sqrt(max(0, 1 - (pm3/pm1))),\n",
    "            pm3/pm1,\n",
    "            L, W, H\n",
    "        ]\n",
    "\n",
    "    def _extract_global_props(self, props_2d, props_3d, charge_val, coords_3d):\n",
    "        props_map = {'logp': 0.0, 'tpsa': 0.0, 'rotatable': 0.0, 'h_acceptor': 0.0, 'h_donor': 0.0, 'volume': 0.0}\n",
    "        for prop in props_2d:\n",
    "            urn = prop.get('urn', {})\n",
    "            label, name, val = urn.get('label', ''), urn.get('name', ''), prop.get('value', {})\n",
    "            if label == 'Log P' and name == 'XLogP3-AA': props_map['logp'] = val.get('fval', 0.0)\n",
    "            elif label == 'Topological' and name == 'Polar Surface Area': props_map['tpsa'] = val.get('fval', 0.0)\n",
    "            elif label == 'Count' and name == 'Rotatable Bond': props_map['rotatable'] = float(val.get('ival', 0))\n",
    "            elif label == 'Count' and name == 'Hydrogen Bond Acceptor': props_map['h_acceptor'] = float(val.get('ival', 0))\n",
    "            elif label == 'Count' and name == 'Hydrogen Bond Donor': props_map['h_donor'] = float(val.get('ival', 0))\n",
    "        for prop in props_3d:\n",
    "            if prop.get('urn', {}).get('name') == 'Volume': props_map['volume'] = prop.get('value', {}).get('fval', 0.0)\n",
    "        shape_feats = self._calculate_shape_descriptors(coords_3d)\n",
    "        features_list = [props_map[k] for k in props_map] + [float(charge_val)] + shape_feats\n",
    "        return torch.tensor(features_list, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    def _extract_partial_charges(self, num_atoms, props_list):\n",
    "        charges = torch.zeros(num_atoms, 1, dtype=torch.float)\n",
    "        for prop in props_list:\n",
    "            if prop.get('urn', {}).get('name') == 'MMFF94 Partial':\n",
    "                for item in prop.get('value', {}).get('slist', []):\n",
    "                    try:\n",
    "                        parts = item.split()\n",
    "                        charges[int(parts[0])-1] = float(parts[1])\n",
    "                    except: pass\n",
    "                break\n",
    "        return charges\n",
    "\n",
    "    def build_molecule_graph(self, cid):\n",
    "        file_3d = os.path.join(Config.CONFORMER_3D_PATH, f\"Conformer3D_COMPOUND_CID_{cid}.json\")\n",
    "        file_2d = os.path.join(Config.STRUCTURE_2D_PATH, f\"Structure2D_COMPOUND_CID_{cid}.json\")\n",
    "        main_data_2d, props_2d = None, []\n",
    "        total_charge = 0.0\n",
    "        smiles_str = None\n",
    "        \n",
    "        if os.path.exists(file_2d):\n",
    "            try:\n",
    "                with open(file_2d, 'r', encoding='utf-8') as f: d = json.load(f)\n",
    "                if 'PC_Compounds' in d:\n",
    "                    main_data_2d = d['PC_Compounds'][0]\n",
    "                    props_2d = main_data_2d.get('props', [])\n",
    "                    total_charge = float(main_data_2d.get('charge', 0.0))\n",
    "\n",
    "                    for prop in props_2d:\n",
    "                        if prop.get('urn', {}).get('label') == 'SMILES':\n",
    "                            smiles_str = prop.get('value', {}).get('sval')\n",
    "                            break\n",
    "            except: pass\n",
    "        \n",
    "        # 2.RDKit Generating diagrams and conformations\n",
    "        if smiles_str:\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles_str)\n",
    "                if mol:\n",
    "                    mol = Chem.AddHs(mol)\n",
    "                    params = AllChem.ETKDGv2()\n",
    "                    params.randomSeed = 42\n",
    "                    cids_rdkit = AllChem.EmbedMultipleConfs(mol, numConfs=Config.NUM_CONFORMERS, params=params)\n",
    "                    \n",
    "                    if len(cids_rdkit) > 0:\n",
    "                        AllChem.MMFFOptimizeMoleculeConfs(mol, numThreads=0)\n",
    "                    \n",
    "                    pos_variants = []\n",
    "                    if len(cids_rdkit) > 0:\n",
    "                        for i in range(len(cids_rdkit)):\n",
    "                            conf = mol.GetConformer(cids_rdkit[i])\n",
    "                            pos = conf.GetPositions()\n",
    "                            pos = pos - np.mean(pos, axis=0)\n",
    "                            pos_variants.append(pos)\n",
    "                    \n",
    "                    if len(pos_variants) > 0:\n",
    "                        while len(pos_variants) < Config.NUM_CONFORMERS:\n",
    "                            pos_variants.append(pos_variants[0])\n",
    "                        \n",
    "                        atom_features = [self._get_rich_atom_features(atom=atom) for atom in mol.GetAtoms()]\n",
    "                        x = torch.tensor(atom_features, dtype=torch.long) # Shape: [N, 6]\n",
    "                        pos_main = torch.tensor(pos_variants[0], dtype=torch.float)\n",
    "                        pos_variants_tensor = torch.tensor(np.array(pos_variants), dtype=torch.float)\n",
    "                        \n",
    "                        edge_indices, edge_weights = [], []\n",
    "                        for bond in mol.GetBonds():\n",
    "                            u, v = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                            dist = np.linalg.norm(pos_main[u].numpy() - pos_main[v].numpy())\n",
    "                            w = 1.0 / (dist + 0.1)\n",
    "                            edge_indices.extend([[u, v], [v, u]])\n",
    "                            edge_weights.extend([w, w])\n",
    "                        \n",
    "                        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous() if edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "                        edge_weight = torch.tensor(edge_weights, dtype=torch.float) if edge_weights else torch.empty(0)\n",
    "                        \n",
    "                        AllChem.ComputeGasteigerCharges(mol)\n",
    "                        charges = [float(atom.GetProp('_GasteigerCharge')) if atom.HasProp('_GasteigerCharge') else 0.0 for atom in mol.GetAtoms()]\n",
    "                        x_charge = torch.tensor(charges, dtype=torch.float).unsqueeze(1)\n",
    "                        \n",
    "                        return Data(\n",
    "                            x=x,\n",
    "                            edge_index=edge_index,\n",
    "                            edge_weight=edge_weight,\n",
    "                            x_charge=x_charge,\n",
    "                            global_attr=self._extract_global_props(props_2d, [], total_charge, pos_main.numpy()),\n",
    "                            pos=pos_main,\n",
    "                            pos_variants=pos_variants_tensor\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                # print(f\"RDKit Build Failed for {cid}: {e}\")\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def build_zeolite_graph(self, topology):\n",
    "        patterns = [os.path.join(Config.CIF_PATH, f\"*{topology}*.cif*\"), os.path.join(Config.CIF_PATH, topology, \"*.cif*\")]\n",
    "        cif_files = []\n",
    "        for p in patterns: cif_files.extend(glob(p))\n",
    "        if not cif_files: return None\n",
    "\n",
    "        try:\n",
    "            struct = Structure.from_file(cif_files[0])\n",
    "            \n",
    "            # PBC\n",
    "            atom_features = []\n",
    "            for site in struct:\n",
    "                feats = self._get_rich_atom_features(element_symbol=site.specie.symbol, is_crystal=True)\n",
    "                atom_features.append(feats)\n",
    "            \n",
    "            x = torch.tensor(atom_features, dtype=torch.long)\n",
    "            pos = torch.tensor(struct.cart_coords, dtype=torch.float)\n",
    "            \n",
    "            # create a border \n",
    "            nbrs = struct.get_all_neighbors(r=Config.CRYSTAL_RADIUS, include_index=True)\n",
    "            \n",
    "            edge_indices, edge_attrs = [], []\n",
    "            for i, nbr_list in enumerate(nbrs):\n",
    "                for nbr in sorted(nbr_list, key=lambda x: x[1])[:12]:\n",
    "                    target_index = nbr[2]\n",
    "                    distance = nbr[1]\n",
    "                    edge_indices.append([i, target_index])\n",
    "                    edge_attrs.append(distance)\n",
    "            \n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_attrs, dtype=torch.float).unsqueeze(1)\n",
    "            \n",
    "            struct_super = struct.copy()\n",
    "            struct_super.make_supercell([3, 3, 3]) \n",
    "            pos_super = torch.tensor(struct_super.cart_coords, dtype=torch.float)\n",
    "            pos_super = pos_super - torch.mean(pos_super, dim=0)\n",
    "\n",
    "            return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, pos=pos, pos_super=pos_super)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "# 4. Data Preprocessing and Caching\n",
    "def build_mol_helper(cid):\n",
    "    return (cid, GraphBuilder().build_molecule_graph(cid))\n",
    "\n",
    "def build_zeo_helper(topo):\n",
    "    return (topo, GraphBuilder().build_zeolite_graph(topo))\n",
    "\n",
    "def prepare_and_cache_data(df):\n",
    "    if os.path.exists(Config.PROCESSED_CACHE_PATH):\n",
    "        print(f\"Discover cache files: {Config.PROCESSED_CACHE_PATH}\")\n",
    "        try:\n",
    "            return torch.load(Config.PROCESSED_CACHE_PATH, weights_only=False)\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load\")\n",
    "    \n",
    "    print(\"No cache files found\")\n",
    "    unique_cids = df['CID'].unique()\n",
    "    unique_topos = df['Topology Code'].unique()\n",
    "\n",
    "    print(f\"  - Number of unique molecules: {len(unique_cids)}\")\n",
    "    print(f\"  - Number of unique zeolites: {len(unique_topos)}\")\n",
    "    \n",
    "    print(\"Constructing molecular maps\")\n",
    "    mol_results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "        delayed(build_mol_helper)(cid) for cid in tqdm(unique_cids, desc=\"Molecules\")\n",
    "    )\n",
    "    mol_cache = {res[0]: res[1] for res in mol_results if res[1] is not None}\n",
    "    \n",
    "    print(\"Constructing a zeolite diagram\")\n",
    "    zeo_results = Parallel(n_jobs=min(len(unique_topos), multiprocessing.cpu_count()))(\n",
    "        delayed(build_zeo_helper)(topo) for topo in tqdm(unique_topos, desc=\"Zeolites\")\n",
    "    )\n",
    "    zeo_cache = {res[0]: res[1] for res in zeo_results if res[1] is not None}\n",
    "    \n",
    "    cache_data = {'mol_cache': mol_cache, 'zeo_cache': zeo_cache}\n",
    "    print(f\"Save the cache to: {Config.PROCESSED_CACHE_PATH}\")\n",
    "    torch.save(cache_data, Config.PROCESSED_CACHE_PATH)\n",
    "    return cache_data\n",
    "\n",
    "# 5. Dataset \n",
    "class ZeoliteDataset(Dataset):\n",
    "    def __init__(self, df, cache_data, target_scaler=None, props_scaler=None, is_train=False):\n",
    "        super().__init__()\n",
    "        self.target_scaler = target_scaler if target_scaler else StandardScaler()\n",
    "        self.props_scaler = props_scaler if props_scaler else StandardScaler()\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        mol_cache = cache_data['mol_cache']\n",
    "        zeo_cache = cache_data['zeo_cache']\n",
    "        \n",
    "        self.mol_list = []\n",
    "        self.zeo_list = []\n",
    "        raw_y_list = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            cid = row['CID']\n",
    "            topo = row['Topology Code']\n",
    "            \n",
    "            if cid in mol_cache and topo in zeo_cache:\n",
    "                targets = row[Config.TARGET_COLS].values.astype(float)\n",
    "                if not np.isnan(targets).any():\n",
    "                    self.mol_list.append(mol_cache[cid])\n",
    "                    self.zeo_list.append(zeo_cache[topo])\n",
    "                    raw_y_list.append(targets)\n",
    "        \n",
    "        y_all = np.array(raw_y_list)\n",
    "        if is_train:\n",
    "            y_norm = self.target_scaler.fit_transform(y_all)\n",
    "        else:\n",
    "            y_norm = self.target_scaler.transform(y_all) if hasattr(self.target_scaler, 'mean_') else y_all\n",
    "            \n",
    "        self.y_list = [torch.tensor(y, dtype=torch.float) for y in y_norm]\n",
    "        \n",
    "        if len(self.mol_list) > 0:\n",
    "            all_props = torch.cat([m.global_attr for m in self.mol_list], dim=0).numpy()\n",
    "            if is_train:\n",
    "                self.props_scaler.fit(all_props)\n",
    "                \n",
    "        self.length = len(self.mol_list)\n",
    "        print(f\"Dataset Finish building: {self.length} sample (Train={is_train})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mol_data = self.mol_list[idx].clone()\n",
    "        zeo_data = self.zeo_list[idx].clone()\n",
    "        y = self.y_list[idx]\n",
    "\n",
    "        if hasattr(self.props_scaler, 'mean_'):\n",
    "            props_raw = mol_data.global_attr.numpy()\n",
    "            props_norm = self.props_scaler.transform(props_raw)\n",
    "            mol_data.global_attr = torch.tensor(props_norm, dtype=torch.float)\n",
    "\n",
    "        mol_coords = mol_data.pos.numpy()\n",
    "        if hasattr(mol_data, 'pos_variants'):\n",
    "            variants = mol_data.pos_variants\n",
    "            if self.is_train:\n",
    "                conf_idx = np.random.randint(len(variants))\n",
    "                mol_coords = variants[conf_idx].numpy()\n",
    "            else:\n",
    "                mol_coords = variants[0].numpy()\n",
    "            del mol_data.pos_variants \n",
    "\n",
    "        zeo_voxel_coords = zeo_data.pos_super.numpy() if hasattr(zeo_data, 'pos_super') else zeo_data.pos.numpy()\n",
    "        if hasattr(zeo_data, 'pos_super'): del zeo_data.pos_super \n",
    "\n",
    "        if self.is_train:\n",
    "            rot_matrix = get_random_rotation_matrix()\n",
    "            mol_coords = np.dot(mol_coords, rot_matrix)\n",
    "            mol_noise = np.random.normal(0, 0.02, mol_coords.shape)\n",
    "            zeo_noise = np.random.normal(0, 0.02, zeo_voxel_coords.shape)\n",
    "            mol_coords += mol_noise\n",
    "            zeo_voxel_coords += zeo_noise\n",
    "            \n",
    "        mol_data.pos = torch.tensor(mol_coords, dtype=torch.float)\n",
    "        \n",
    "        grid_mol = coords_to_voxel(mol_coords, Config.VOXEL_SIZE, Config.VOXEL_RES, Config.SIGMA)\n",
    "        grid_zeo = coords_to_voxel(zeo_voxel_coords, Config.VOXEL_SIZE, Config.VOXEL_RES, Config.SIGMA)\n",
    "        \n",
    "        voxel_tensor = torch.tensor(np.stack([grid_mol, grid_zeo], axis=0), dtype=torch.float)\n",
    "        \n",
    "        return mol_data, zeo_data, voxel_tensor, y\n",
    "\n",
    "    @staticmethod\n",
    "    def gpu_collate(batch):\n",
    "        mol_list = [item[0] for item in batch]\n",
    "        zeo_list = [item[1] for item in batch]\n",
    "        voxel_list = [item[2] for item in batch]\n",
    "        y_list = [item[3] for item in batch]\n",
    "        \n",
    "        return (Batch.from_data_list(mol_list),\n",
    "                Batch.from_data_list(zeo_list),\n",
    "                torch.stack(voxel_list),\n",
    "                torch.stack(y_list))\n",
    "\n",
    "# 6. Model Definition\n",
    "class Voxel3DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(32)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm3d(64)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        \n",
    "        self.fc = nn.Linear(64 * 8 * 8 * 8, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "class DualBranchGNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_atom = nn.Embedding(120, Config.ATOM_EMBEDDING_DIM)\n",
    "        self.emb_degree = nn.Embedding(12, Config.EMB_DIM_DEGREE)\n",
    "        self.emb_charge = nn.Embedding(15, Config.EMB_DIM_CHARGE)\n",
    "        self.emb_hyb = nn.Embedding(8, Config.EMB_DIM_HYB)\n",
    "        self.emb_aromatic = nn.Embedding(2, Config.EMB_DIM_AROMATIC)\n",
    "        self.emb_chiral = nn.Embedding(4, Config.EMB_DIM_CHIRAL)\n",
    "        \n",
    "        total_emb_dim = (Config.ATOM_EMBEDDING_DIM + Config.EMB_DIM_DEGREE + \n",
    "                         Config.EMB_DIM_CHARGE + Config.EMB_DIM_HYB + \n",
    "                         Config.EMB_DIM_AROMATIC + Config.EMB_DIM_CHIRAL)\n",
    "        \n",
    "        self.mol_conv1 = GCNConv(total_emb_dim + 1, Config.HIDDEN_DIM)\n",
    "        self.mol_conv2 = GCNConv(Config.HIDDEN_DIM, Config.HIDDEN_DIM)\n",
    "        \n",
    "        self.zeo_conv1 = GCNConv(total_emb_dim, Config.HIDDEN_DIM)\n",
    "        self.zeo_conv2 = GCNConv(Config.HIDDEN_DIM, Config.HIDDEN_DIM)\n",
    "        \n",
    "        self.voxel_cnn = Voxel3DCNN()\n",
    "        \n",
    "        self.global_feat_dim = 17\n",
    "        self.global_encoder = nn.Sequential(\n",
    "            nn.Linear(self.global_feat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, Config.HIDDEN_DIM),\n",
    "            nn.BatchNorm1d(Config.HIDDEN_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        fusion_dim = Config.HIDDEN_DIM * 4\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, len(Config.TARGET_COLS))\n",
    "        )\n",
    "\n",
    "    def _embed_features(self, x_idx):\n",
    "        e1 = self.emb_atom(x_idx[:, 0])\n",
    "        e2 = self.emb_degree(x_idx[:, 1])\n",
    "        e3 = self.emb_charge(x_idx[:, 2])\n",
    "        e4 = self.emb_hyb(x_idx[:, 3])\n",
    "        e5 = self.emb_aromatic(x_idx[:, 4])\n",
    "        e6 = self.emb_chiral(x_idx[:, 5])\n",
    "        return torch.cat([e1, e2, e3, e4, e5, e6], dim=1)\n",
    "\n",
    "    def forward(self, mol_batch, zeo_batch, voxel_batch):\n",
    "        # --- molecules GNN ---\n",
    "        x_m, edge_index_m, batch_m = mol_batch.x, mol_batch.edge_index, mol_batch.batch\n",
    "        x_m_emb = self._embed_features(x_m)\n",
    "        x_m_in = torch.cat([x_m_emb, mol_batch.x_charge], dim=1)\n",
    "        \n",
    "        x_m_out = F.relu(self.mol_conv1(x_m_in, edge_index_m, edge_weight=mol_batch.edge_weight))\n",
    "        x_m_out = F.relu(self.mol_conv2(x_m_out, edge_index_m, edge_weight=mol_batch.edge_weight))\n",
    "        feat_m = global_mean_pool(x_m_out, batch_m)\n",
    "        \n",
    "        # --- zeolites GNN ---\n",
    "        x_z, edge_index_z, batch_z = zeo_batch.x, zeo_batch.edge_index, zeo_batch.batch\n",
    "        x_z_emb = self._embed_features(x_z)\n",
    "        \n",
    "        x_z_out = F.relu(self.zeo_conv1(x_z_emb, edge_index_z))\n",
    "        x_z_out = F.relu(self.zeo_conv2(x_z_out, edge_index_z))\n",
    "        feat_z = global_mean_pool(x_z_out, batch_z)\n",
    "        \n",
    "        # --- 3D CNN ---\n",
    "        feat_v = self.voxel_cnn(voxel_batch)\n",
    "        \n",
    "        # --- global feature ---\n",
    "        global_attr = mol_batch.global_attr\n",
    "        if global_attr.dim() == 3: global_attr = global_attr.squeeze(1)\n",
    "        feat_global = self.global_encoder(global_attr)\n",
    "        \n",
    "        combined = torch.cat([feat_m, feat_z, feat_global, feat_v], dim=1)\n",
    "        return self.head(combined)\n",
    "\n",
    "# 7. main program\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_state_dict = model.state_dict()\n",
    "            return False\n",
    "\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_state_dict = model.state_dict()\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"precession count: {self.counter}/{self.patience} (Best: {self.best_loss:.6f})\")\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.freeze_support()\n",
    "    \n",
    "    if not os.path.exists(Config.DATASET_PATH):\n",
    "        print(f\"Error: File does not exist {Config.DATASET_PATH}\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_excel(Config.DATASET_PATH, engine='openpyxl')\n",
    "\n",
    "    print(f\"Total raw data: {len(df)}\")\n",
    "    topo_counts = df['Topology Code'].value_counts()\n",
    "    \n",
    "    valid_topos = topo_counts[topo_counts >= Config.MIN_SAMPLES_PER_TOPO].index\n",
    "    \n",
    "    df_filtered = df[df['Topology Code'].isin(valid_topos)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Total amount of data after cleaning: {len(df_filtered)} (decline {len(df) - len(df_filtered)} samples, thresholds={Config.MIN_SAMPLES_PER_TOPO})\")\n",
    "    print(f\"Types of zeolites retained: {len(valid_topos)}\")\n",
    "\n",
    "    cache_data = prepare_and_cache_data(df_filtered)\n",
    "    \n",
    "    indices = list(range(len(df_filtered)))\n",
    "    train_idx, temp_idx = train_test_split(indices, train_size=0.8, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, train_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"\\nData set segmentation: train {len(train_idx)} | validation {len(val_idx)} | test {len(test_idx)}\")\n",
    "    \n",
    "    train_dataset = ZeoliteDataset(df_filtered.iloc[train_idx].reset_index(drop=True), cache_data=cache_data, is_train=True)\n",
    "    val_dataset = ZeoliteDataset(df_filtered.iloc[val_idx].reset_index(drop=True), cache_data=cache_data, \n",
    "                                 target_scaler=train_dataset.target_scaler, props_scaler=train_dataset.props_scaler, is_train=False)\n",
    "    test_dataset = ZeoliteDataset(df_filtered.iloc[test_idx].reset_index(drop=True), cache_data=cache_data, \n",
    "                                  target_scaler=train_dataset.target_scaler, props_scaler=train_dataset.props_scaler, is_train=False)\n",
    "\n",
    "    loader_kwargs = {\n",
    "        'batch_size': Config.BATCH_SIZE,\n",
    "        'num_workers': Config.NUM_WORKERS,\n",
    "        'pin_memory': Config.PIN_MEMORY,\n",
    "        'collate_fn': ZeoliteDataset.gpu_collate,\n",
    "        'persistent_workers': Config.NUM_WORKERS > 0\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, **loader_kwargs)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DualBranchGNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=Config.SCHEDULER_FACTOR, \n",
    "        patience=Config.SCHEDULER_PATIENCE, \n",
    "        min_lr=Config.MIN_LR\n",
    "    )\n",
    "    \n",
    "    early_stopper = EarlyStopping(patience=Config.EARLY_STOPPING_PATIENCE, min_delta=Config.EARLY_STOPPING_MIN_DELTA)\n",
    "    \n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    print(f\"\\nstart.\")\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.EPOCHS}\", unit=\"batch\")\n",
    "        \n",
    "        for mol, zeo, voxel, y in pbar:\n",
    "            mol, zeo, voxel, y = mol.to(device), zeo.to(device), voxel.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                pred = model(mol, zeo, voxel)\n",
    "                loss = criterion(pred, y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mol, zeo, voxel, y in val_loader:\n",
    "                mol, zeo, voxel, y = mol.to(device), zeo.to(device), voxel.to(device), y.to(device)\n",
    "                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                    pred = model(mol, zeo, voxel)\n",
    "                    val_loss += criterion(pred, y).item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        if early_stopper(avg_val_loss, model):\n",
    "            print(f\"\\n Early Stop ! Best : {early_stopper.best_loss:.4f}\")\n",
    "            model.load_state_dict(early_stopper.best_state_dict)\n",
    "            break\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train')\n",
    "        plt.plot(val_losses, label='Val')\n",
    "        plt.title(f'Loss (Epoch {epoch+1}) - Best Val: {early_stopper.best_loss:.4f}')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train={avg_train_loss:.4f}, Val={avg_val_loss:.4f}, LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    print(\"\\nFinal evaluation...\")\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for mol, zeo, voxel, y in test_loader:\n",
    "            mol, zeo, voxel, y = mol.to(device), zeo.to(device), voxel.to(device), y.to(device)\n",
    "            preds.append(model(mol, zeo, voxel).cpu().numpy())\n",
    "            targets.append(y.cpu().numpy())\n",
    "    \n",
    "    y_pred = train_dataset.target_scaler.inverse_transform(np.vstack(preds))\n",
    "    y_true = train_dataset.target_scaler.inverse_transform(np.vstack(targets))\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    for i, col in enumerate(Config.TARGET_COLS):\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        mae = np.mean(np.abs(y_true[:, i] - y_pred[:, i]))\n",
    "        print(f\"{col[:25]:25} : RÂ² = {r2:.4f}, MAE = {mae:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"zeolite_3d_gnn_enriched_cleaned.pth\")\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721641c-d33a-4fa8-ace3-92ae9640685b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
